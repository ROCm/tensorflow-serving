# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

ARG TF_SERVING_VERSION=latest
ARG TF_SERVING_BUILD_IMAGE=rocm/tensorflow-serving:${TF_SERVING_VERSION}-centos7-devel

FROM ${TF_SERVING_BUILD_IMAGE} as build_image
FROM centos:centos7.9.2009

ARG TF_SERVING_VERSION_GIT_BRANCH=master-QA-stable-rocm60
ARG TF_SERVING_VERSION_GIT_COMMIT=c4baf862f6d44e421fd25cb043755375377eddff
ARG DEBIAN_FRONTEND=noninteractive

LABEL maintainer="Jason.Furmanek@amd.com"
LABEL tensorflow_serving_github_branchtag=${TF_SERVING_VERSION_GIT_BRANCH}
LABEL tensorflow_serving_github_commit=${TF_SERVING_VERSION_GIT_COMMIT}

ARG ROCM_ROM_REPO=https://repo.radeon.com/amdgpu/5.7/rhel/7.9/main/x86_64
ARG ROCM_BUILD_NAME=ubuntu
ARG ROCM_BUILD_NUM=main
ARG ROCM_PATH=/opt/rocm-5.7.0

ARG DEBIAN_FRONTEND=noninteractive
ENV TF_NEED_ROCM 1
ENV HOME /root/
ENV HIP_PLATFORM=amd

COPY amdgpu.repo /etc/yum.repos.d/
COPY rocm.repo /etc/yum.repos.d/
RUN yum install -y epel-release
RUN yum repolist
RUN yum clean all
RUN yum install -y \
        ca-certificates \
        wget \
        swig \
        libtool \
        automake \
        curl \
        unzip \
        zip \
        pkg-config \
        perl-File-BaseDir \
        perl-URI-Encode \
        rocm-dev  \
        rocm-libs \
        rccl \
        miopenkernels*

# Install TF Serving pkg
COPY --from=build_image /usr/local/bin/tensorflow_model_server /usr/bin/tensorflow_model_server

# Expose ports
# gRPC
EXPOSE 8500

# REST
EXPOSE 8501

# Set where models should be stored in the container
ENV MODEL_BASE_PATH=/models
RUN mkdir -p ${MODEL_BASE_PATH}

# The only required piece is the model name in order to differentiate endpoints
ENV MODEL_NAME=model

# Create a script that runs the model server so we can use environment variables
# while also passing in arguments from the docker command line
RUN echo -e '#!/bin/bash \n\n\
tensorflow_model_server --port=8500 --rest_api_port=8501 \
--model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \
"$@"' > /usr/bin/tf_serving_entrypoint.sh \
&& chmod +x /usr/bin/tf_serving_entrypoint.sh

ENTRYPOINT ["/usr/bin/tf_serving_entrypoint.sh"]
